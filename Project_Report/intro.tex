\section{Introduction}
In today's Deep Learning scheme, the presumption of the training is that all instances of the classes are available, the model will iterate the dataset per epoch to learn the knowledge. This means if there are new instances added to the dataset, we have to retrain the whole model on the whole dataset plus the new instances. This has caused trouble in some scenarios. Specifically, in image classification problems, the pretrained model often encounters new objects or the dataset can be expanded. However, the existence of \textit{Catastrophic Forgetting}\cite{mccloskey1989catastrophic}, i.e. the newly learned parameters will shift the old parameters and weaken its performance on old task, has brought challenge to this problem. This phenomenon can greatly degrade the performance of the model or even totally rewrite the model's parameters, causing the old knowledge being totally forgot. In before, in face of such case, one have to choose either retrain the model on entire dataset or just tolerate such degrading issues. To overcome the dilemma, the concept of continual learning(Lifelong Learning\cite{thrun1995lifelong}) has emerged. In continual learning, it is required that the model must have not only the ability to acquire new knowledge, but also prevent the novel input to overwhelm the original data. 

Continual learning is close to the concept of online learning. But in online learning setting, there will be some dependency on the previous data, while in continual learning, we do not want to access the original dataset because it is costly and time consuming. The focus of continual learning is not only on maintaining the accuracy, but also on training efficiently. Currently, there are three main approaches to apply continual learning,
\begin{enumerate}
\item \textbf{Retraining}
Totally retraining the old parameters $\theta_o$ on the new task to obtain the new model $\theta_n$, and then apply regularization to prevent the degrading problem on old task. The typical work in this approach are \textit{Learning without Forgetting}(LwF)\cite{li2017learning} and  Elastic Weight Consolidation(EWC)\cite{kirkpatrick2017overcoming}. 
\item \textbf{Expansion}
Non-Retraining but instead expanding the network. Whenever encounter the new task, just freeze the old weight and expand the network. One of the typical work is \textit{Progressive Network}\cite{rusu2016progressive}. Another variation of this work is masking, i.e. they focus not only on the weigth itsely, but also considered masking some of the unimportant weights for the new task $t$. Packenet\cite{mallya2018packnet} and Piggyback\cite{mallya2018piggyback} adopt this approach.
\item \textbf{Partial Retraining with Expansion}
Another approach is selectively retraining the old network, expanding the capacity when necessary and train dynamically on the optimal solution. One of the outstanding work in this approach is \textit{Dynamically Expandable Networks}(DEN)\cite{yoon2017lifelong}.
\end{enumerate}

In this project, we focus on the \textit{CLVision Challenge}\footnote{https://sites.google.com/view/clvision2020/challenge?authuser=0}, i.e. Continual Learning in Computer Vision, a problem set published by CVPR 2020 Workshop to solve the image classification problem in continual learning. We have reproduced and refined several famous approaches, including \textit{EWC}, \textit{Piggyback} and \textit{DEN}(?), and tried out them on the CLVision dataset, then we evaluate the performance and analyze the potential improvements that can be done. 