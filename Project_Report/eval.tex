\section{Evaluation and Analyze}\label{eval}
In this part, we evaluate the performance of \textit{EWC} and \textit{Piggyback}. We will discuss the performance for them on NI(New Instance) case and NIC(New Instance and Class) case respectively. Then we will analyze their performance case by case. According to the guideline of the \textit{CLVision Challenge}, we use average accuracy, i.e. the mean accuracy per task as the metrics. Besides, metrics including maximum RAM usage, average RAM usage and training time should also be taken into account to validate the performance.
\begin{table}
\caption{Average Accuracy Performance for EWC/Piggyback}
\label{performance}
\centering
\begin{tabular}{c|cccc}
	\toprule
	Scenario & Baseline(ResNet50) & EWC & Piggyback(ResNet50) & Piggyback(EWC Combined) \\
	\midrule
	NI-val & 0.54 & 0.59 & 0.21 & 0.62 \\
	\midrule
	NI-test & 0.71 & 0.64 & - & 0.74 \\
	\midrule
	NIC-val & 0.46 & 0.52 & 0.10 & 0.53 \\
	\midrule
	NIC-test & 0.83 & 0.89 & - & 0.92 \\
	\bottomrule
\end{tabular}
\end{table}

\subsection{EWC}
\textcolor{red}{PlaceHolder}

\subsection{Piggyback}
As mentioned in Section \ref{design}, the choice of a well-trained base network is crucial to the performance of $Piggyback$. In this part, all of our experiments are done in a PC with CPU i7 8700K, 32GB RAM, and RTX 2080 GPU with 8GB SDRAM. In all our cases, we set batch size to 32, with learning rate at $1e-4$ and decay one tenth every 5 epochs. At the very first, we have followed the setting in the paper, but failed to reproduce the result. We have tried out ImageNet-pretrained VGG16\cite{simonyan2014very}, DenseNet\cite{huang2017densely} and ResNet50 as base network but the best average accuracy we can reach is 0.1037 in NIC case and 0.2103 in NI case, which is even far worse than the baseline. Considering the principle of Piggyback, we deem that it is the worse performance of the orignal model that causes the bad performance for Piggyback. We test the accuracy before training and it is only 0.0148. This worse performance could be caused by the under-training of the fully connected layer in these three models. As mentioned before, there are 50 classes in our dataset, considering these 50 classes are not similar to what it is in the pre-trained network, there pre-trained weights do not work well on predicting the label of new task. Therefore, modification of the mask can partially solve the problem, but the core cause is the poor performance of the original weight.

To solve this problem, we modified the training procedure. We still load the pre-trained weights for ResNet50 and let the model run EWC at the first task. Then, at the proceeding task, we create mask layer $\mathbf{m_r}$ for the model, and run Piggyback. In this approach, we have raised the final average validation accuracy to 0.53, which is slightly better than our baseline. 

\subsection{Analyze}
\textcolor{red}{PlaceHolder}
